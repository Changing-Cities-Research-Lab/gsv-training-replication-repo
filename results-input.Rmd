---
title: "Results"
author: "XXX"
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r summary}
# Script Description ------------------------------------------------------

# Use this script to generate the tables that are in the training data paper.

# Inputs:
# Pairwise data - pairs.csv
# Single Image dataset - single_image.csv

```


```{r packages}

# Libraries and Directories
rm(list=ls())
library("gtools")
library("tidyverse")
library("readr")
library("irr") #for agreement, reliability
library("foreach")

# Directories:
homedir <- paste0(dirname(rstudioapi::getSourceEditorContext()$path))  # path to where script is located, in the main repository folder
workdir <- "data/"  # data folder
savedir <- paste0(homedir, workdir)

# Import data:
setwd(paste0(homedir, workdir))



```

# Load Data and Set Appropriate Column Types
```{r data}

single_image <- read_csv("single_image.csv",
                         guess_max = 1000000) # read_csv will use the first 1,000 rows to guess type, but since this is missing for a lot of columns, they are incorrectly called in as logical. So, we want to maximize the number of guesses. This will slow down read_csv, but necessary for import.


pairs <- read_csv("pairs.csv",
                  guess_max = 1000000)

```

# Tables

## Table 1: Number of Unique Images Coded by MTurkers

```{r images-mturkers}
single_results <- single_image %>% 
  filter(!is.na(single_mturk_rating1)) %>% # Keep only those that have at least 1 MTurk rating
  group_by(city) %>% 
  summarize(
    single_image_coders = mean(unique_mturkers,na.rm=TRUE), # Calculate the number of unique MTurk coders 
    single_images_coded = n_distinct(image_name) # Calculate the number of unique images coded
  )

pairs_results <- pairs %>% 
  filter(!is.na(pairs_mturk_choice1)) %>% # Keep only those that have at least 1 MTurk rating
  mutate(ID = group_indices(.,image_name1,image_name2)) %>% # Create group idetnifier for each pair - based on image 1 and image 2
  group_by(city) %>% 
  filter(!grepl("_MA_",image_name1)) %>% #remove earlier pairwise Boston results, irrelevant for analysis here.
  summarize(
    pairs_coders = mean(unique_mturkers,na.rm=TRUE), # Calculate the number of unique MTurk coders 
    pairs_coded = n_distinct(ID)  # Calculate the number of unique images coded
  ) %>% 
  dplyr::select(-city)

cbind(single_results,pairs_results) # Put together for table

rm(single_results,pairs_results)
```

## Table 2 : Distribution of Trash in Training Data

```{r training-data}

single_image %>% 
  filter(training==1) %>% # Keep only those in the training set
  group_by(city,trueskill_rating) %>% # Group by city and each category
  summarize(n = n()) %>% # Count by category/city
  mutate(percent = n/sum(n)*100) # Calculate percentage in each category


```

## Table 3 : Distribution of ML Predictions of Trash
```{r ml-distributions}

single_image %>%
  filter(!is.na(pred_svm)) %>% 
  group_by(city) %>%
  summarize(pred_0 = sum(pred_svm==0, na.rm = T), pred_1 = sum(pred_svm==1, na.rm = T), total = n()) %>%
  mutate(perc = pred_1/total)


```

## Table 4 : Counts of Images from Coding Sessions
```{r images-coders}
single_results <- single_image %>% 
  filter(!is.na(single_cs_rating1)) %>% 
  group_by(city) %>% 
  summarize(
    single_image_coders = mean(unique_coders,na.rm=TRUE),
    single_images_coded = n_distinct(image_name)
  )

pairs_results <- pairs %>% 
  filter(!is.na(pairs_cs_choice1)) %>% 
  mutate(ID = group_indices(.,image_name1,image_name2)) %>% 
  group_by(city) %>% 
  summarize(
    pairs_coders = mean(unique_coders,na.rm=TRUE),
    pairs_coded = n_distinct(ID)
  ) %>% 
  dplyr::select(-city)

cbind(single_results,pairs_results)

rm(single_results,pairs_results)
```

## Table 5 : Pair Comparisons Reliability Panel 1
```{r pairwise-reliability-betweenraters}
cities <- c("Boston", "Detroit", "LA")

pairs_cities <- 
  foreach (i = 1:length(cities)) %do% {
    pairs %>% 
      filter(city %in% cities[i]) %>% # subset to each each city
      select(city, which(grepl("pairs_cs_choice", names(pairs)))) %>% #subset to coding session ratings
      filter_at(vars(pairs_cs_choice1:pairs_cs_choice4), any_vars(!is.na(.))) %>% # remove if no ratings
      rowwise() %>%
      mutate(n_raters = sum(!is.na(c_across(where(is.numeric))))) %>% # number of raters per pair
      filter(n_raters > 1) # only keep pairs rated by 2+ people
  }
names(pairs_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table5_1 <- 
  foreach (i = 1:length(pairs_cities),
           .combine = rbind.data.frame) %do% {
             foreach (j = 1:length(unique(pairs_cities[[i]]$n_raters)), 
                      .combine = rbind.data.frame) %do% {
                        # subset data for each number of raters
                        dat <- 
                          pairs_cities[[i]] %>%
                          filter(n_raters == sort(unique(pairs_cities[[i]]$n_raters))[j])
                        # output a vector with number of raters, subjects, and each value
                        out <- 
                          c(
                            # city
                            cities[i], 
                            # number of raters
                            unique(pairs_cities[[i]]$n_raters)[j], 
                            # number of pairs
                            agree(dat %>% select(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1)))$subjects,
                            # % agreement
                            agree(dat %>% select(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1)))$value,
                            # Intraclass correlation
                            icc(dat %>% select(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1)), 
                                model = "twoway")$value,
                            # krippendorf's alpha
                            kripp.alpha(
                              t(as.matrix(dat %>% select(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1)))), 
                              "ordinal")$value
                          )
                        out <- as.data.frame(t(as.data.frame(out))) 
                        names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
                        return(out)
                        rm(out)
                      }
           }
table5_1 <- table5_1 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table5_1 <- table5_1 %>% filter(n > 2) # remove if too few comparisons n <= 2

# add overall pct agreement (weighted) for each city
table5_1 <- 
  bind_rows(
    table5_1, # merge with original table
    table5_1 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )

```

### Table 5 : Pair Comparisons Reliability Panel 2

```{r pairwise-reliability-betweenratersandmturk}

# Create list with separate dataframes for each city among coding sessions with consensus
pairs_cities <- 
  foreach (i = 1:length(cities)) %do% {
    pairs %>% 
      filter(city %in% cities[i]) %>% # subset to each each city
      select(city, which(grepl("pairs_cs_choice", names(pairs))), 
             which(grepl("pairs_mturk_choice", names(pairs)))) %>% #subset to coding session and mturk ratings
      filter_at(vars(pairs_cs_choice1:pairs_cs_choice4), any_vars(!is.na(.))) %>% # remove if no ratings
      rowwise() %>%
      mutate(n_raters = sum(!is.na(c_across(where(is.numeric)))) - 1, # number of student raters per pair
             var = var(c_across(starts_with("pairs_cs_choice")), na.rm = TRUE)) %>%  # calculate variance across pairs to see consensus
      filter(var==0 | (is.na(var) & !is.na(pairs_cs_choice1))) #Keep those with no variance (i.e. equal) or NA variance (i.e. one rater)
  }
names(pairs_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table5_2 <- 
  foreach (
    i = 1:length(pairs_cities),
    .combine = rbind.data.frame) %do% {
      foreach (
        j = 1:length(unique(pairs_cities[[i]]$n_raters)), 
        .combine = rbind.data.frame) %do% {
          # subset data for each number of raters
          dat <- 
            pairs_cities[[i]] %>%
            filter(n_raters == sort(unique(pairs_cities[[i]]$n_raters))[j])
          # create output 
          out <- 
            c(
              # city
              cities[i], 
              # number of raters
              sort(unique(pairs_cities[[i]]$n_raters))[j], 
              # number of pairs
              agree(dat %>% select(c(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1), 6)))$subjects,
              # % agreement
              agree(dat %>% select(c(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1), 6)))$value,
              # Intraclass correlation
              icc(dat %>% select(c(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1), 6)), 
                  model = "twoway")$value,
              # krippendorf's alpha
              kripp.alpha(
                t(as.matrix(
                  dat %>% select(c(2:(2+sort(unique(pairs_cities[[i]]$n_raters))[j] - 1), 6)))), 
                "ordinal")$value)
          out <- as.data.frame(t(as.data.frame(out))) 
          names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
          return(out)
          rm(out)
        }
    }
table5_2 <- table5_2 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table5_2 <- table5_2 %>% filter(n > 3) # remove if too few comparisons n > 3

# add overall pct agreement (weighted) for each city
table5_2 <- 
  bind_rows(
    table5_2, # merge with original table
    table5_2 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )

```

## Table 6 : Single Image Reliability Panel 1
```{r single-reliability-betweenraters, warning=FALSE}

# Create list with separate dataframes for each city 
ratings_cities <- 
  foreach (
    i = 1:length(cities)) %do% {
      single_image %>% 
        filter(city %in% cities[i]) %>% # subset to each each city
        select(city, which(grepl("single_cs_rating", names(single_image)))) %>% #subset to coding session ratings
        filter_at(vars(single_cs_rating1:single_cs_rating7), any_vars(!is.na(.))) %>% # remove if no ratings
        rowwise() %>%
        mutate(n_raters = sum(!is.na(c_across(where(is.numeric))))) %>% # number of raters per image
        filter(n_raters > 1) %>% # only keep images rated by 2+ people
        mutate(n_raters = ifelse(n_raters > 4, 4, n_raters)) # a few images have more than 4 raters, just analyze first 4 ratings
    }
names(ratings_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table6_1 <- 
  foreach (
    i = 1:length(ratings_cities),
    .combine = rbind.data.frame) %do% {
      foreach (
        j = 1:length(unique(ratings_cities[[i]]$n_raters)), 
        .combine = rbind.data.frame) %do% {
          # subset data for each number of raters
          dat <- 
            ratings_cities[[i]] %>%
            filter(n_raters == sort(unique(ratings_cities[[i]]$n_raters))[j])
          # output a vector with number of raters, images, and each value
          out <- 
            c(# city
              cities[i], 
              # number of raters
              sort(unique(ratings_cities[[i]]$n_raters))[j], 
              # number of images
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))$subjects,
              # % agreement
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))$value,
              # Intraclass correlation
              icc(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)), 
                  model = "twoway")$value,
              # krippendorf's alpha
              kripp.alpha(
                t(as.matrix(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))), 
                "ordinal")$value)
          out <- as.data.frame(t(as.data.frame(out))) 
          names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
          return(out)
          rm(out)
        }
    }
colnames(table6_1) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
table6_1 <- table6_1 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table6_1 <- table6_1 %>% filter(n > 3) # remove if too few comparisons >3

# add overall pct agreement (weighted) for each city
table6_1 <- 
  bind_rows(
    table6_1, # merge with original table
    table6_1 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )
```

### Table 6: Single Image Reliability Panel 2

```{r single-reliability-betweenratersandmturk}
# Create list with separate dataframes for each city among coding sessions with consensus
ratings_cities <- 
  foreach (
    i = 1:length(cities)) %do% {
      single_image %>% 
        filter(city %in% cities[i]) %>% # subset to each each city
        select(city, which(grepl("single_cs_rating", names(single_image))), 
               trueskill_rating) %>% #subset to coding session ratings and trueskill ratings
        filter_at(vars(single_cs_rating1:single_cs_rating7), any_vars(!is.na(.))) %>% # remove if no ratings
        rowwise() %>%
        mutate(n_raters = sum(!is.na(c_across(where(is.numeric)))) - 1, # number of raters per image
               var = var(c_across(starts_with("single_cs_rating")), na.rm = TRUE)) %>%  # calculate variance across pairs to see consensus) 
        filter(var==0 | (is.na(var) & !is.na(single_cs_rating1))) %>%  #Keep those with no variance (i.e. equal) or NA variance (i.e. one rater)
        filter(n_raters > 1) %>% # only keep images rated by 2+ people
        mutate(n_raters = ifelse(n_raters > 4, 4, n_raters)) # a few images have more than 4 raters, just analyze first 4 ratings
    }
names(ratings_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table6_2 <- 
  foreach (
    i = 1:length(ratings_cities),
    .combine = rbind.data.frame) %do% {
      foreach (
        j = 1:length(unique(ratings_cities[[i]]$n_raters)), 
        .combine = rbind.data.frame) %do% {
          # subset data for each number of raters
          dat <- 
            ratings_cities[[i]] %>%
            filter(n_raters == sort(unique(ratings_cities[[i]]$n_raters))[j])
          # output a vector with number of raters, images, and each value
          out <- 
            c(# city
              cities[i], 
              # number of raters
              sort(unique(ratings_cities[[i]]$n_raters))[j], 
              # number of images
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))$subjects,
              # % agreement
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))$value,
              # Intraclass correlation
              icc(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9), 
                  model = "twoway")$value,
              # krippendorf's alpha
              kripp.alpha(
                t(as.matrix(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))), 
                "ordinal")$value)
          out <- as.data.frame(t(as.data.frame(out))) 
          names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
          return(out)
          rm(out)
        }
    }
colnames(table6_2) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
table6_2 <- table6_2 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table6_2 <- table6_2 %>% filter(n > 3) # remove if too few comparisons >3

# add overall pct agreement (weighted) for each city
table6_2 <- 
  bind_rows(
    table6_2, # merge with original table
    table6_2 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )
```

## Table 7 : Reliability Between Raters and Predictions Panel 1
```{r single-reliability-betweenraters}

# Create list with separate dataframes for each city with at least 2 ratings 
ratings_cities <- 
  foreach (
    i = 1:length(cities)) %do% {
      single_image %>% 
        filter(city %in% cities[i]) %>% # subset to each each city
        select(city, which(grepl("single_cs_rating", names(single_image)))) %>% #subset to coding session ratings
        filter_at(vars(single_cs_rating1:single_cs_rating4), any_vars(!is.na(.))) %>% # remove if no ratings
        rowwise() %>%
        mutate_at(
          vars(starts_with("single_cs_rating")), # convert ratings to binary version
          funs(case_when(
            . %in% c(0,1) ~ 0, # (0, 1) = obstructed or no trash
            . %in% c(2,3,4) ~ 1 # (2, 3, 4) = little, some, or a lot of trash
          ))) %>% 
        mutate(n_raters = sum(!is.na(c_across(where(is.numeric))))) %>% # number of raters per image
        filter(n_raters > 1) %>% # only keep images rated by 2+ people
        mutate(n_raters = ifelse(n_raters > 4, 4, n_raters)) # a few images have more than 4 raters, just analyze first 4 ratings
    }
names(ratings_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table7_1 <- 
  foreach (
    i = 1:length(ratings_cities),
    .combine = rbind.data.frame) %do% {
      foreach (
        j = 1:length(unique(ratings_cities[[i]]$n_raters)), 
        .combine = rbind.data.frame) %do% {
          # subset data for each number of raters
          dat <- 
            ratings_cities[[i]] %>%
            filter(n_raters == sort(unique(ratings_cities[[i]]$n_raters))[j])
          # output a vector with number of raters, images, and each value
          out <- 
            c(# city
              cities[i], 
              # number of raters
              sort(unique(ratings_cities[[i]]$n_raters))[j], 
              # number of images
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))$subjects,
              # % agreement
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))$value,
              # Intraclass correlation
              icc(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)), 
                  model = "twoway")$value,
              # krippendorf's alpha
              kripp.alpha(
                t(as.matrix(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1)))), 
                "ordinal")$value)
          out <- as.data.frame(t(as.data.frame(out))) 
          names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
          return(out)
          rm(out)
        }
    }
colnames(table7_1) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
table7_1 <- table7_1 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table7_1 <- table7_1 %>% filter(n > 3) # remove if too few comparisons >3

# add overall pct agreement (weighted) for each city
table7_1 <- 
  bind_rows(
    table7_1, # merge with original table
    table7_1 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )
```

### Table 7: Reliability Between Raters and Predictions Panel 2

```{r single-reliability-betweenratersandmturk}
# Create list with separate dataframes for each city among coding sessions with consensus
ratings_cities <- 
  foreach (
    i = 1:length(cities)) %do% {
      single_image %>% 
        filter(city %in% cities[i] & # subset to each each city
                 !is.na(pred_svm)) %>% # remove if missing ML prediction
        select(city, which(grepl("single_cs_rating", names(single_image))), 
               pred_svm) %>% #subset to coding session ratings and ML predictions
        filter_at(vars(single_cs_rating1:single_cs_rating7), any_vars(!is.na(.))) %>% # remove if no ratings
        rowwise() %>%
        mutate_at(
          vars(starts_with("single_cs_rating")), # convert ratings to binary version
          funs(case_when(
            . %in% c(0,1) ~ 0, # (0, 1) = obstructed or no trash
            . %in% c(2,3,4) ~ 1 # (2, 3, 4) = little, some, or a lot of trash
          ))) %>% 
        mutate(n_raters = sum(!is.na(c_across(where(is.numeric)))) - 1, # number of raters per image
               var = var(c_across(starts_with("single_cs_rating")), na.rm = TRUE)) %>%  # calculate variance across pairs to see consensus) 
        filter(var==0 | (is.na(var) & !is.na(single_cs_rating1))) %>% #Keep those with no variance (i.e. equal) or NA variance (i.e. one rater)
        filter(n_raters > 1) %>% # only keep images rated by 2+ people
        mutate(n_raters = ifelse(n_raters > 4, 4, n_raters)) # a few images have more than 4 raters, just analyze first 4 ratings
    }
names(ratings_cities) <- cities

# for each city, calculate % agreement, intraclass correlation, and krippendorf's alpha  
table7_2 <- 
  foreach (
    i = 1:length(ratings_cities),
    .combine = rbind.data.frame) %do% {
      foreach (
        j = 1:length(unique(ratings_cities[[i]]$n_raters)), 
        .combine = rbind.data.frame) %do% {
          # subset data for each number of raters
          dat <- 
            ratings_cities[[i]] %>%
            filter(n_raters == sort(unique(ratings_cities[[i]]$n_raters))[j])
          # output a vector with number of raters, images, and each value
          out <- 
            c(# city
              cities[i], 
              # number of raters
              sort(unique(ratings_cities[[i]]$n_raters))[j], 
              # number of images
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))$subjects,
              # % agreement
              agree(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))$value,
              # Intraclass correlation
              icc(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9), 
                  model = "twoway")$value,
              # krippendorf's alpha
              kripp.alpha(
                t(as.matrix(dat %>% select(2:(2+sort(unique(ratings_cities[[i]]$n_raters))[j] - 1), 9))), 
                "ordinal")$value)
          out <- as.data.frame(t(as.data.frame(out))) 
          names(out) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
          return(out)
          rm(out)
        }
    }
colnames(table7_2) <- c("city", "n_raters", "n", "pct_agree", "icc", "kripp_alpha")
table7_2 <- table7_2 %>% mutate_at(-c(1:2), as.numeric) # convert columns to numeric
table7_2 <- table7_2 %>% filter(n > 3) # remove if too few comparisons >3

# add overall pct agreement (weighted) for each city
table7_2 <- 
  bind_rows(
    table7_2, # merge with original table
    table7_2 %>% 
      group_by(city) %>% 
      # calculate overall pct agreement (Weighted for each city)
      summarise(
        overall_n = sum(n),
        overall_pct_agree = weighted.mean(pct_agree, n)
      ) %>% 
      # add columns for merge
      mutate(n_raters = "overall", icc = NA, kripp_alpha = NA) %>%
      # modify column names for merge
      rename(
        n = overall_n, 
        pct_agree = overall_pct_agree
      )
  )
```

## Trueskill v. Coding Session Figure
```{r fig-3}

# Aggregate Trueskill ratings
trueskill_agg <- single_image %>% 
  filter(!is.na(trueskill_rating)) %>% # Keep only those that have one Trueskill rating
  group_by(city,trueskill_rating) %>% 
  summarize(count = n()) %>% # Within each city-rating grouping, count how many there are
  mutate(perc = (count/sum(count))) %>% # Get percentage of each rating
  mutate(type = "Trueskill") %>% 
  rename(rating = trueskill_rating) # Rename to match other ratings

# Aggregate Trueskill ratings
cs_agg <- single_image %>% 
  dplyr::select(image_name,city,starts_with("single_cs_rating")) %>%
  filter(!is.na(single_cs_rating1)) %>% # Keep only those that have at least one coding session rating
  pivot_longer( # Reshape so that each coding session rating is its own row
    cols = starts_with("single_cs_rating"),
    values_to = "rating"
  ) %>% 
  filter(!is.na(rating) & !rating==0) %>% # Keep only those that don't have an obstructed rating
  group_by(city,rating) %>% 
  summarize(count = n()) %>% 
  mutate(perc = (count/sum(count))) %>% 
  mutate(type = "Coding Session") 

# Bind Trueskill and coding session ratings together
agg <- rbind(cs_agg,trueskill_agg)

# Get overall rates (not city disaggregated)
agg_overall <- agg %>% 
  group_by(type,rating) %>% 
  summarize(count = sum(count)) %>% 
  mutate(perc = (count/sum(count)))

#Plot each panel
#Overall----
agg_overall_plot <-
ggplot(agg_overall, aes(x=rating, y=perc,fill=type)) +
  geom_bar(position = "dodge", stat="identity") +
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.25) +
  scale_y_continuous(labels = scales::percent) + 
  coord_cartesian(ylim=c(0,0.80)) + 
  labs(title = "All Cities", y = "Percent", x="Rating", fill = "Rating Source") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#Boston----
agg_bos_plot <-
  agg %>% 
  filter(city == "Boston") %>% 
  ggplot(aes(x=rating, y=perc,fill=type)) +
  geom_bar(position = "dodge", stat="identity") +
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.25) +
  scale_y_continuous(labels = scales::percent) + 
  coord_cartesian(ylim=c(0,0.80)) + 
  labs(title = "Boston", y = "Percent", x="Rating", fill = "Rating Source") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#Detroit----
agg_det_plot <-
  agg %>% 
  filter(city == "Detroit") %>% 
  ggplot(aes(x=rating, y=perc,fill=type)) +
  geom_bar(position = "dodge", stat="identity") +
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.25) +
  scale_y_continuous(labels = scales::percent) + 
  coord_cartesian(ylim=c(0,0.80)) + 
  labs(title = "Detroit", y = "Percent", x="Rating", fill = "Rating Source") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#LA----
agg_la_plot <-
  agg %>% 
  filter(city == "LA") %>% 
  ggplot(aes(x=rating, y=perc,fill=type)) +
  geom_bar(position = "dodge", stat="identity") +
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.25) +
  scale_y_continuous(labels = scales::percent) + 
  coord_cartesian(ylim=c(0,0.80)) + 
  labs(title = "Los Angeles", y = "Percent", x="Rating", fill = "Rating Source") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

# Figure----
fig <- ggpubr::ggarrange(agg_overall_plot, agg_bos_plot, agg_det_plot, agg_la_plot,
                                        ncol = 2, nrow = 2,
                    common.legend = TRUE,
                    legend = "bottom")




```

## Coding Session v. Predictions Figure

```{r fig-4}
# Aggregate coding session ratings (binary)
cs_agg_binary <- single_image %>% 
  dplyr::select(image_name,city,starts_with("single_cs_rating")) %>% 
  filter(!is.na(single_cs_rating1)) %>% 
  pivot_longer(
    cols = starts_with("single_cs_rating"),
    values_to = "rating"
  ) %>% 
  filter(!is.na(rating) & !rating==0) %>% 
  mutate(rating = ifelse(rating > 1, 1, 0)) %>% 
  group_by(city,rating) %>% 
  summarize(count = n()) %>% 
  mutate(perc = (count/sum(count))) %>% 
  mutate(type = "Coding Session") %>% 
  mutate(rating = factor(rating,
                         labels = c("No Trash", "Trash"))) %>% 
  mutate(rating = relevel(rating, ref = "No Trash"))

# Aggregate predictions
preds_agg <- single_image %>% 
  filter(!is.na(pred_svm)) %>% # Keep only those with a prediction
  group_by(city,pred_svm) %>% 
  summarize(count = n()) %>% 
  mutate(perc = (count/sum(count))) %>% 
  mutate(type = "ML") %>% 
  rename(rating = pred_svm) %>% 
  mutate(rating = factor(rating,
                         labels = c("No Trash", "Trash"))) %>% 
  mutate(rating = relevel(rating, ref = "No Trash"))

# Bind predictions and coding sessions together
agg <- rbind(preds_agg,cs_agg_binary)

# Get overall rates (not city disaggregated)
agg_overall <- agg %>% 
  group_by(type,rating) %>% 
  summarize(count = sum(count)) %>% 
  mutate(perc = (count/sum(count)))

#Plot each panel

#Overall----
agg_overall_plot <-
ggplot(agg_overall, aes(x=type, y=perc,fill=rating)) +
  geom_bar(position = "stack", stat="identity") +
  scale_y_continuous(labels = scales::percent) + 
  labs(title = "All Cities", y = "Percent", x="Rating Source", fill = "Rating") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#Boston----
agg_bos_plot <-
  agg %>% 
  filter(city == "Boston") %>% 
  ggplot(aes(x=type, y=perc,fill=rating)) +
  geom_bar(position = "stack", stat="identity") +
  scale_y_continuous(labels = scales::percent) + 
  labs(title = "Boston", y = "Percent", x="Rating Source", fill = "Rating") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#Detroit----
agg_det_plot <-
  agg %>% 
  filter(city == "Detroit") %>% 
   ggplot(aes(x=type, y=perc,fill=rating)) +
  geom_bar(position = "stack", stat="identity") +
  scale_y_continuous(labels = scales::percent) + 
  labs(title = "Detroit", y = "Percent", x="Rating Source", fill = "Rating") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#LA----
agg_la_plot <-
  agg %>% 
  filter(city == "LA") %>% 
  ggplot(aes(x=type, y=perc,fill=rating)) +
  geom_bar(position = "stack", stat="identity") +
  scale_y_continuous(labels = scales::percent) + 
  labs(title = "Los Angeles", y = "Percent", x="Rating Source", fill = "Rating") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_manual(values = c("gray40", "gray70"))

#Figure----
fig <- ggpubr::ggarrange(agg_overall_plot, agg_bos_plot, agg_det_plot, agg_la_plot,
                                        ncol = 2, nrow = 2,
                    common.legend = TRUE,
                    legend = "bottom") 


```